





import sys
print(sys.executable)



# Import libraries

import pandas as pd
import time
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import matplotlib.pyplot as plt 
import os
import logging


#installing the driver
service = Service(ChromeDriverManager().install())

# starts Chrome using Service
driver = webdriver.Chrome(service=service)


#Get the page's contents
page_url = "https://en.wikipedia.org/wiki/Alice%27s_Adventures_in_Wonderland"
driver.get(page_url)





# Create a collection of the characters
characters_elems = driver.find_elements(by = By.CLASS_NAME, value = 'div-col')





list_char = characters_elems[0].text.split("\n")





list_char


#testing
print(len(characters_elems))








# Put the characters into a dataframe
df = pd.DataFrame(list_char, columns = ["character"])


df





#import libraries
from bs4 import BeautifulSoup
import requests 








# Assistant
# First, install the required package
!pip install beautifulsoup4

# Then import libraries
from bs4 import BeautifulSoup
import requests


#import libraries
from bs4 import BeautifulSoup
import requests 


headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}


page_url_2 = requests.get("https://en.wikipedia.org/wiki/Alice%27s_Adventures_in_Wonderland", headers=headers)


soup = BeautifulSoup(page_url_2.text, 'html.parser')
print(soup.title)





text = soup.get_text()


text = text.encode ('utf-8')


with open('Alice_article_Wiki.txt', 'wb') as f:
       f.write(text)






